### 주식 가격 분석 모델 개발 일지

<br/>

### 2023.7.24 - 2023.8.3

#### 진행사항
  - 주가 데이터 수집(보조지표 포함)과 전처리 및 metadata 형식으로 정리 진행
  - 주가 분석 모델 툴킷 제작 및 모델 설계 (CNN + RNN 기반) 진행
  - 하이퍼 파라미터 설정 및 학습 진행

<br/>

### 2023.8.6

#### 현재 문제점
  - 현재, 주가 분석 모델 툴킷을 제작하여 훈련을 진행하고 있는 중이다.
  - 70 epochs 를 최종 목표로 하고 있는 도중, 10, 20, 30 epochs 일 때의 Training 과 Validation 에 대한 MSE 와 Loss 를 보면서 모델 품질에 대하여 판단하는 중이다.
  - 그 결과, Training 에 대해서는 mse 가 대폭 줄어들고 있지만, Validation 에 대해서는 mse 가 줄고 있지 않는 상태를 발견하였다.

#### 예상되는 원인
  - 현재 metadata.txt 에서 시장과 종목 코드를 기준으로 정렬하게 해두었는데, 이로 인하여 Validation 쪽에 변동폭이 큰 종목들이 대거 들어가게 되어 그렇게 된 것 같다.

#### 데이터에 대한 고찰 (23.8.6)

- 가격 움직임에 대하여 변동성이 큰 종목과 변동성이 작은 종목에 대한 비율이 비슷한가?
  - 변동성에 대한 지표를 바탕으로 변동성이 큰 종목과 변동성이 작은 종목에 대한 비율을 조사한다. 이것도 히스토그램으로 얼마나 분포되어 있는지 확인할 수 있어야 함.
  
- 수익률이 기존 가격에 근접한 종목과 큰 수익을 달성하는 종목의 비율이 어떻게 구성되어 있나?
  - 히스토그램을 바탕으로 수익에 있어서 10% 단위에 대하여 데이터의 분산을 확인한다. 
  - 만약, 비율이 다르다면 어떻게 진행하는 것이 좋을 것 같나?
  - 일단, train 과 validation 데이터를 골고루 분산시키는 것이 좋겠다. metadata.txt 에 있는 데이터들을 기반으로 각 데이터들에 대하여 sort 를 진행하고 만약 train 과 validation 사이의 관계가 10:1 이라면 (전체데이터개수)/11 (=x) 을 계산하여 10x 를 train 데이터에 넣고, x 를 validation 데이터에 집어넣는 방식으로 진행.
  - 비교적 변동성이 큰 종목의 데이터 개수가 모자랄 것이라는 생각이 든다. 이에 대하여 데이터 개수가 부족한 부분에 대하여 데이터 증강을 통하여 접근할 수 있도록하면 좋겠다는 생각을 하였다.
  
- 데이터 증강의 경우, 정확하게 어떤 방식을 선택할 것인가?
  - 현재, Time Domain 에 있어서 기본적으로 제안하는 증강 기법은 Window cropping or slicing, Window warping, Flipping, Noise injection, Label expansion 정도가 있다. 나에게 적합한 방식은 Noise Injection 가 될 것 같다. 노이즈로는 gaussian noise, spike, step-like trend, and slope-like trend 정도가 있다고 한다. 최종적으로 gaussian noise 를 적용할 예정이다.

<br/>

### 2023.8.10

#### EDA 접근
![화면 캡처 2023-08-13 031115](https://github.com/DevTae/StockDatabasePreview/assets/55177359/b9340afa-88bd-4cf8-b92b-9c0c3dbb580b)
- 결과층 데이터가 특정 구간에 몰려 있는 것을 확인 / 이러한 부분이 데이터 불균형을 뜻하고 학습 평가 검증이 다소 어려워질 수 있음
- 따라서, 결과값 확대 및 축소 함수(특정 공식)를 적용하여 다음과 같이 **정규 분포 형태의 데이터 분포**로 구성할 수 있었다.
- 이전의 문제점을 해결할 수 있어 당장에는 Data Augmentation 을 적용하지 않고 학습을 진행하고자 한다.
- 데이터 수는 약 160만 개로 충분히 커버가 가능할 듯 보인다.

<br/>

### 2023.08.19

#### 모델 구조 변경
  - CNN + RNN 결합한 모델을 바탕으로 학습을 돌려본 결과, 생각보다 좋은 성능이 나오질 않았다. (약 5~6% 내외의 오차율을 보임 / MSE 1000, MAE 28 정도 (평가지표의 경우, 결과층에 의도적으로 Scaling 을 진행하지 않아 이런 결과가 나오게 되었음))
  - LSTM 을 바탕으로 Normalization 기법까지 적용하여 모델을 구성하였고 학습을 시작할 수 있었다.
  - 평가지표를 MSE 에서 MAE 로 변경하였음. (Loss Function 이 MSE 로 설정되어 있어, 평가지표와 같은 값을 나타내게 되어 변경하였음)

<br/>

### 2023.08.20

#### 하이퍼파라미터 튜닝 및 전처리 스케일링 방식 변경
  - 모델 구조를 바꿨음에도 MSE 가 1000 부근으로 나옴으로 하이퍼파라미터 설정과 스케일링 방식을 변경하기로 하였다.
  - 학습률, Epoch, RNN 레이어 개수, Hidden Dimension 에 대한 하이퍼파라미터 튜닝을 진행하였다.
  - 마지막 종가를 바탕으로 모든 수치를 계산하지 않고 이전 종가를 기준으로 수치를 스케일링하여 변환하였다.
  
<br/>

### 2023.08.21 ~ 2023.08.22

#### 현재 학습 결과
  - 새롭게 결과층에 Scaling 을 해제한 값을 적용하여 `PriceMeanSquaredError` 평가지표를 구현하고 적용하였다.
    - 20 Epochs 에 PMSE 가 39.19 (이전 MSE 와 기준이 다름) 정도가 나오며 평균적으로 6.2% 의 오차를 가지고 있다는 것을 알 수 있다.
    - 이전 CNN + RNN 결합한 모델과 그렇게 큰 차이를 보이진 않는 것으로 보였다. 
    - 현재 오차 목표를 3% 정도로 잡았고, 이에 따라 PMSE 가 10 정도가 되는 것이 목표이다.

#### 이후 방향성
  - 스케일링 방식을 바탕으로 Feature 들에 대하여 Non-Stationary 데이터가 되도록 변경할 것이다.
  - 하이퍼 파라미터 튜닝을 계속하여 진행할 것이다.

<br/>

### 2023.08.23 ~ 2023.08.25

#### 현재 진행 상황
  - 이전 데이터셋은 모양만 정규분포와 유사하고 Shapiro-Wilk 검정을 진행한 결과, 정규분포가 나오지 않게 되어 이에 대한 데이터 보정을 진행하고자 함.
    - 그 중에서도, 정규분포보다 균등분포에 더 가깝도록 데이터를 설계하고자 하였음.
    - QQ Plot 을 진행한 결과, 다음과 같이 개선을 할 수 있었음.
      ![image](https://github.com/DevTae/StockDatabasePreview/assets/55177359/a66c4bc9-f106-4cb1-b83a-c44932a73f2d)


